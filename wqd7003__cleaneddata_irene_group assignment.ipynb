{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import timestamp\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Dropout, BatchNormalization\n",
    "from sklearn.model_selection import KFold\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.model_selection import GridSearchCV   #Performing grid search\n",
    "from sklearn.model_selection import validation_curve\n",
    "import gc\n",
    "pd.options.mode.chained_assignment= None  \n",
    "pd.options.display.max_columns = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Data \n",
    "file_train = \"../train_v2.csv\"\n",
    "file_test = \"../test_v2.csv\"\n",
    "chunk_size = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file, chunk_size, nrows_load=None, test_data=False):\n",
    "    df_res = pd.DataFrame()\n",
    "    df_reader = pd.read_csv(file, dtype={ 'date': str, 'fullVisitorId': str}, chunksize=10000)\n",
    "    \n",
    "    for cidx, df in enumerate(df_reader):\n",
    "        df.reset_index(drop=True, inplace=True)   \n",
    "        process_df(df, test_data)\n",
    "        df_res = pd.concat([df_res,df ], axis=0).reset_index(drop=True)\n",
    "        del df #free memory\n",
    "        gc.collect()\n",
    "        #print every 20 iterations\n",
    "        if cidx % 20 == 0:\n",
    "            print('{}: rows loaded: {}'.format(cidx, df_res.shape[0]))\n",
    "        if nrows_load:\n",
    "            if res.shape[0] >= nrows_load:\n",
    "                break\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every column as key and the important features to extract from each column\n",
    "def parse_json(x,s):\n",
    "    res = json.loads(x)\n",
    "    try:\n",
    "        return res[s]\n",
    "    except:\n",
    "        return float('NaN') \n",
    "\n",
    "    \n",
    "def process_df(df,test_data):\n",
    "    #process date \n",
    "    df['days'] = df['date'].str[-2:]\n",
    "    df['days'] = df['days'].astype(int)\n",
    "    df['month'] = df['date'].str[-4:-2]\n",
    "    df['month'] = df['month'].astype(int)\n",
    "    df['year'] = df['date'].str[:4]\n",
    "    df['year'] = df['year'].astype(int)\n",
    "    df['visitStartTime'] = df['visitStartTime'].astype('datetime64[s]')\n",
    "    \n",
    "    #process json fields\n",
    "    process_dict = {\n",
    "        'totals':['transactionRevenue','newVisits','pageviews','hits'] ,\n",
    "        'trafficSource':['campaign','source','medium'] ,\n",
    "        'device':['browser'],\n",
    "        'geoNetwork': ['country','city','continent','region','subContinent']\n",
    "    }\n",
    " \n",
    "    #add new columns from json in df\n",
    "    for c,l in process_dict.items():\n",
    "        for it in l:\n",
    "            df[it] = df[c].apply(lambda x : parse_json(x,it))\n",
    "    \n",
    "    #process time\n",
    "    colA = ['visitStartTime']\n",
    "    for v in colA:\n",
    "        df.sort_values([\"visitStartTime\"], axis=0, ascending=True, inplace=True)     \n",
    "    \n",
    "    #labelencoding for continuous data\n",
    "    cols = ['country','campaign','source','medium','continent','city','region','socialEngagementType','browser'\n",
    "             ,'channelGrouping','subContinent','date']\n",
    "    labelencoder_X=LabelEncoder()\n",
    "    for c in cols:\n",
    "        df.loc[:,c] = labelencoder_X.fit_transform(df.loc[:,c])\n",
    "            \n",
    "    #Dealing with missing values\n",
    "    #transactionsRevenue and NewVisits:  nans ->  0\n",
    "    df['transactionRevenue'].fillna(0,inplace=True)\n",
    "    df['newVisits'].fillna(0,inplace=True)\n",
    "    df['pageviews'].fillna(0,inplace=True)\n",
    "    \n",
    "    \n",
    "    #Casting Str columns to int\n",
    "    df['transactionRevenue'] = df['transactionRevenue'].astype('float32')\n",
    "    df['newVisits']= df['newVisits'].astype('uint16')\n",
    "    df['pageviews'] = df['pageviews'].astype('uint16')\n",
    "    df['hits'] = df['hits'].astype('uint32')\n",
    "    #df['index'] = df['index'].astype('uint32')\n",
    "                                            \n",
    "    #remove json field columns and some unwanted columns\n",
    "    #(some removed for saving memory)\n",
    "    rm_col = ['subContinent','channelGrouping','date','continent','customDimensions','fullVisitorId']\n",
    "    if test_data:\n",
    "        rm_col = rm_col[:-1]\n",
    "    df.drop(list(process_dict.keys()) + rm_col, axis=1,inplace=True)\n",
    "    \n",
    "#load and process\n",
    "df = load_data(file_train,chunk_size)\n",
    "df_test =load_data(file_test,chunk_size,test_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkmissingvalues(df):\n",
    "    return[df.isnull().sum()]\n",
    "print(checkmissingvalues(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkmissingvalues(df_test):\n",
    "    return[df_test.isnull().sum()]\n",
    "print(checkmissingvalues(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to train_cleaned.csv\n",
    "df.to_csv(path_or_buf=\"../train_cleaned.csv\", chunksize = 10000, index=False, index_label=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export to test_cleaned.csv\n",
    "df_test.to_csv(path_or_buf=\"../test_cleaned.csv\", chunksize = 10000, index=False, index_label=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
